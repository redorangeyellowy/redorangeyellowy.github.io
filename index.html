<html lang="en">
    <!DOCTYPE HTML>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Junho Park</title>

    <meta name="author" content="Junho Park">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon"> -->
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Junho Park
                </p>
                <p>
                  I'm an AI researcher in Vision Intelligence Lab, led by <a href="https://www.linkedin.com/in/jaechul-kim-56919523/">Jaechul Kim</a>, at <a href="https://www.lgcorp.com/">AI Lab, LG Electronics</a>.
                  At LG Electronics, I've worked on Large-Scale Generative Datasets, Vision Foundation Models (e.g. Object Detection, Panoptic Segmentation, Depth Estimation, Pose Estimation, and Face Recognition), and On-Device (e.g. Lightweight Modeling and Quantization).
                  <br><br>
                  I completed my Master's program at <a href="https://www.sogang.ac.kr/en/home">Sogang University</a> advised by <a href="https://vds.sogang.ac.kr/">Suk-Ju Kang</a>, and closely collaborated with <a href="https://www.pnu-cvsp.com/">Kyeongbo Kong</a>.
                  At Sogang University, I've worked on Diffusion Models, Large Language Models, Egocentric Vision, Hand-Object Interaction, Pose/Gaze Estimation, Image Restoration, and Machine Learning.
                  <br><br>
                  Additionally, I'm independently pursuing research on AR/VR, Embodied AI, and Robot Learning with <a href="https://taeinkwon.com/">Taein Kwon</a>.
                  <br><br>
                  I'm open to collaboration—feel free to reach out!
                </p>
                <p style="text-align:center">
                  <a href="mailto:junho18.park@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=1OSw79kAAAAJ&hl=ko">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/junho18-park/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/redorangeyellowy/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="data/profile.jpg"><img style="width:100%;max-width:100%;object-fit: cover;" alt="profile photo" src="data/profile.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>News</h2>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
            <tr>
              <td style="padding:16px; width:100%; vertical-align:middle">
                [Jan. 2026] Our paper <a href="https://redorangeyellowy.github.io/EgoWorld/">EgoWorld</a> is accepted to <strong>ICLR 2026</strong>.
                <br>
                [Sep. 2025] Our papers <a href="https://hands-workshop.org/files/2025/Replace-in-Ego.pdf">Replace-in-Ego</a> and <a href="https://hands-workshop.org/files/2025/Generating.pdf">GenEgo</a> are accepted to <strong>ICCV 2025 Workshop</strong>.
                <br>
                [Jan. 2025] Our paper <a href="https://jihyun0510.github.io/Programmable_Room_Page/">Programmable-Room</a> is accepted to <strong>IEEE TMM</strong>.
                <br>
                [Sep. 2024] Our paper <a href="https://sites.google.com/view/cv4metaverse-2024/program?authuser=0">IRP</a> is selected as <font color="red"><strong>Oral Presentation</strong></font> to <strong>ECCV 2024 Workshop</strong>.
                <br>
                [Sep. 2024] Our papers <a href="https://sites.google.com/view/cv4metaverse-2024/program?authuser=0">IRP</a> and <a href="https://hands-workshop.org/workshop2024.html">IHPT</a> are accepted to <strong>ECCV 2024 Workshop</strong>.
                <br>
                [Aug. 2024] Our paper <a href="https://redorangeyellowy.github.io/AttentionHand/">AttentionHand</a> is selected as <font color="red"><strong>Oral Presentation</strong></font> to <strong>ECCV 2024</strong>.
                <br>
                [Jul. 2024] Our paper <a href="https://redorangeyellowy.github.io/AttentionHand/">AttentionHand</a> is accepted to <strong>ECCV 2024</strong>.
                <br>
                [Mar. 2024] I will start my AI researcher position at <a href="https://www.lgcorp.com/">AI Lab, LG Electronics</a>.
                <br>
                [Feb. 2024] Our paper <a href="https://ieeexplore.ieee.org/document/10444704">SEMixup</a> is accepted to <strong>IEEE TIM</strong>.
                <br>
                [Aug. 2023] Our paper <a href="https://sites.google.com/view/hands2023/abstract-report">HANDiffusion</a> is accepted to <strong>ICCV 2023 Workshop</strong>.
                <br>
                [Jun. 2023] Our paper <a href="https://ieeexplore.ieee.org/document/10143197?source=authoralert">SAAF</a> is accepted to <strong>IEEE Access</strong>.
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <!-- <tr onmouseout="cat4d_stop()" onmouseover="cat4d_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='cat4d_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/cat4d.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/attentionhand.png' width="160">
                </div>
                <script type="text/javascript">
                  function cat4d_start() {
                    document.getElementById('cat4d_image').style.opacity = "1";
                  }

                  function cat4d_stop() {
                    document.getElementById('cat4d_image').style.opacity = "0";
                  }
                  cat4d_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://redorangeyellowy.github.io/AttentionHand/">
              <span class="papertitle">AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild
              </span>
                </a>
                <br>
                <strong>Junho Park*</strong>,
                <a href="https://scholar.google.com/citations?hl=ko&user=O9QSF7UAAAAJ">Kyeongbo Kong*</a>,
                <a href="https://scholar.google.com/citations?user=3WYxpuYAAAAJ&hl=ko&oi=ao/">Suk-Ju Kang</a>
                <br>
                <em>ECCV</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://redorangeyellowy.github.io/AttentionHand/">Project Page</a>
                /
                <a href="https://arxiv.org/abs/2407.18034">arXiv</a>
                <p></p>
                <p>
                We propose a novel method, AttentionHand, for text-driven controllable hand image generation. The performance of 3D hand mesh reconstruction was improved by additionally training with hand images generated by AttentionHand.
                </p>
              </td>
            </tr> -->

            <!-- <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src='data/EgoWorld.png' width="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://redorangeyellowy.github.io/">
                  <span class="papertitle">EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations</span>
                </a>
                <br>
                <strong>Junho Park</strong>,
                <a href="https://www.linkedin.com/in/andrew-sangwoo-ye-97a175199/">Andrew Sangwoo Ye</a>,
                <a href="https://taeinkwon.com/">Taein Kwon†</a>
                <br>
                <em>arXiv</em>, 2025 &nbsp
                <br>
                <a href="https://jihyun0510.github.io/Programmable_Room_Page/">Project Page</a>
                /
                Paper (will be published)
                <p></p>
                <p>
                We introduce EgoWorld, a novel two-stage framework that reconstructs egocentric view from rich exocentric observations, including depth maps, 3D hand poses, and textual descriptions. 
                </p>
              </td>
            </tr> -->

            <tr>
              <td style="padding:16px;width:30%;vertical-align:middle">
                <img src='data/EgoWorld.png' style="width:100%; aspect-ratio: 4/3; object-fit: contain; border-radius: 8px;">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://redorangeyellowy.github.io/EgoWorld/">
                  <span class="papertitle">EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations</span>
                </a>
                <br>
                <strong>Junho Park</strong>,
                <a href="https://www.linkedin.com/in/andrew-sangwoo-ye-97a175199/">Andrew Sangwoo Ye</a>,
                <a href="https://taeinkwon.com/">Taein Kwon†</a>
                (†: Corresponding Author.)
                <br>
                <em>ICLR</em>, 2026 &nbsp
                <br>
                <a href="https://redorangeyellowy.github.io/EgoWorld/">Project Page</a>
                /
                <a href="https://arxiv.org/abs/2506.17896">Paper</a>
                <p></p>
                <p style="margin: 0;">
                  We introduce EgoWorld, a novel two-stage framework that reconstructs egocentric view from rich exocentric observations, including depth maps, 3D hand poses, and textual descriptions. 
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:30%;vertical-align:middle">
                <img src='data/TransHOI.png' style="width:100%; aspect-ratio: 4/3; object-fit: contain; border-radius: 8px;">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://redorangeyellowy.github.io/">
                  <span class="papertitle">TransHOI: Implicit 3D-Aware Cross-View Translation for Hand-Object Interaction Generation</span>
                </a>
                <br>
                <strong>Junho Park*</strong>,
                <a href="https://www.linkedin.com/in/yeieun/">Yeieun Hwang*</a>,
                <a href="https://vds.sogang.ac.kr/">Suk-Ju Kang†</a>
                (*: Equal Contribution, †: Corresponding Author.)
                <br>
                <em>Under Review</em>, 2025 &nbsp
                <br>
                <!-- <a href="https://jihyun0510.github.io/Programmable_Room_Page/">Project Page</a>
                / -->
                Paper (will be published)
                <p></p>
                <p>
                We introduce TransHOI, a novel framework for implicit 3D-aware image translation of hand-object interaction, aiming to generate images from different perspectives while preserving appearance details based on user's description of camera.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:30%;vertical-align:middle">
                <img src='data/SQ-Pose.png' style="width:100%; aspect-ratio: 4/3; object-fit: contain; border-radius: 8px;">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://redorangeyellowy.github.io/">
                  <span class="papertitle">Single Query to Bind Them: Unified Representations for Efficient Human Pose Estimation</span>
                </a>
                <br>
                Jonghyun Kim,
                Yubin Yoon,
                Bo-Sang Kim,
                Hyoyoung Kim,
                <strong>Junho Park</strong>,
                Jungho Lee†,
                <a href="https://www.linkedin.com/in/jaechul-kim-56919523/">Jaechul Kim†</a>
                (†: Corresponding Author.)
                <br>
                <em>Under Review</em>, 2025 &nbsp
                <br>
                <!-- <a href="https://jihyun0510.github.io/Programmable_Room_Page/">Project Page</a>
                / -->
                Paper (will be published)
                <p></p>
                <p>
                We propose a novel method to explicitly encode bounding box and keypoint locations in a single query and learn their interactions through multi-head attention and feed-forward network.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:30%;vertical-align:middle">
                <img src='data/Replace_in_Ego.png' style="width:100%; aspect-ratio: 4/3; object-fit: contain; border-radius: 8px;">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://hands-workshop.org/files/2025/Replace-in-Ego.pdf">
                  <span class="papertitle">Replace-in-Ego: Text-Guided Object Replacement in Egocentric Hand-Object Interaction</span>
                </a>
                <br>
                Minsuh Song*,
                <strong>Junho Park*</strong>,
                <a href="https://vds.sogang.ac.kr/">Suk-Ju Kang†</a>
                (*: Equal Contribution, †: Corresponding Author.)
                <br>
                <em>ICCV, 9th Workshop on Observing and Understanding Hands in Action</em>, 2025 &nbsp
                <br>
                <a href="https://hands-workshop.org/files/2025/Replace-in-Ego.pdf">Paper</a>
                <p></p>
                <p style="margin: 0;">
                  We introduce a text-guided object replacement framework, Replace-in-Ego, which integrates a vision-language model (VLM)-based segmentation model with a diffusion transformer (DiT).
                </p>
              </td>
            </tr>
            
            <tr>
              <td style="padding:16px;width:30%;vertical-align:middle">
                <img src='data/GenEgo.png' style="width:100%; aspect-ratio: 4/3; object-fit: contain; border-radius: 8px;">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://hands-workshop.org/workshop2025.html">
                  <span class="papertitle">Generating Egocentric View from Exocentric View via Multimodal Observations</span>
                </a>
                <br>
                <strong>Junho Park</strong>,
                <a href="https://www.linkedin.com/in/andrew-sangwoo-ye-97a175199/">Andrew Sangwoo Ye</a>,
                <a href="https://taeinkwon.com/">Taein Kwon†</a>
                (†: Corresponding Author.)
                <br>
                <em>ICCV, 9th Workshop on Observing and Understanding Hands in Action</em>, 2025 &nbsp
                <br>
                <a href="https://hands-workshop.org/workshop2025.html">Paper</a>
                <p></p>
                <p style="margin: 0;">
                  We introduce GenEgo, a novel two-stage framework that generates an egocentric view from multimodal exocentric observations, including projected point clouds, 3D hand poses, and textual descriptions.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:30%;vertical-align:middle">
                <img src='data/ProgrammableRoom.png' style="width:100%; aspect-ratio: 4/3; object-fit: contain; border-radius: 8px;">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://jihyun0510.github.io/Programmable_Room_Page/">
                  <span class="papertitle">Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models</span>
                </a>
                <br>
                <a href="https://www.linkedin.com/in/%EC%A7%80%ED%98%84-%EA%B9%80-a994171b5/">Jihyun Kim*</a>,
                <strong>Junho Park*</strong>,
                <a href="https://www.pnu-cvsp.com/">Kyeongbo Kong*</a>,
                <a href="https://vds.sogang.ac.kr/">Suk-Ju Kang†</a>
                (*: Equal Contribution, †: Corresponding Author.)
                <br>
                <em>IEEE TMM (Transactions on Multimedia, IF: 9.7)</em>, 2025 &nbsp
                <br>
                <a href="https://jihyun0510.github.io/Programmable_Room_Page/">Project Page</a>
                /
                <a href="https://www.arxiv.org/abs/2506.17707">Paper</a>
                <p></p>
                <p>
                Programmable-Room interactively creates and edits textured 3D meshes given user-specified language instructions. Using pre-defined modules, it translates the instruction into python codes which is executed in an order.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:30%;vertical-align:middle">
                <img src='data/AttentionHand.png' style="width:100%; aspect-ratio: 4/3; object-fit: contain; border-radius: 8px;">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://redorangeyellowy.github.io/AttentionHand/">
                  <span class="papertitle">AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild</span>
                </a>
                <br>
                <strong>Junho Park*</strong>,
                <a href="https://www.pnu-cvsp.com/">Kyeongbo Kong*</a>,
                <a href="https://vds.sogang.ac.kr/">Suk-Ju Kang†</a>
                (*: Equal Contribution, †: Corresponding Author.)
                <br>
                <em>ECCV</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation, Acceptance Rate: 2.3%)</strong></font>
                <br>
                <a href="https://redorangeyellowy.github.io/AttentionHand/">Project Page</a>
                /
                <a href="https://arxiv.org/abs/2407.18034">Paper</a>
                <p></p>
                <p>
                We propose a novel method, AttentionHand, for text-driven controllable hand image generation. The performance of 3D hand mesh reconstruction was improved by additionally training with hand images generated by AttentionHand.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:30%;vertical-align:middle">
                <img src='data/IRP.png' style="width:100%; aspect-ratio: 4/3; object-fit: contain; border-radius: 8px;">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://sites.google.com/view/cv4metaverse-2024/program?authuser=0">
                  <span class="papertitle">Interactive 3D Room Generation for Virtual Reality via Compositional Programming</span>
                </a>
                <br>
                <a href="https://www.linkedin.com/in/%EC%A7%80%ED%98%84-%EA%B9%80-a994171b5/">Jihyun Kim*</a>,
                <strong>Junho Park*</strong>,
                <a href="https://www.pnu-cvsp.com/">Kyeongbo Kong*</a>,
                <a href="https://vds.sogang.ac.kr/">Suk-Ju Kang†</a>
                (*: Equal Contribution, †: Corresponding Author.)
                <br>
                <em>ECCV, 3rd Computer Vision for Metaverse Workshop</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="https://drive.google.com/file/d/19Proh5jl0Ojz6PI4xJzpI15um2xGofbf/view?usp=drive_link">Paper</a>
                <p></p>
                <p>
                We introduce a novel framework, Interactive Room Programmer (IRP), which allows users to conveniently create and modify 3D indoor scenes using natural language. 
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:30%;vertical-align:middle">
                <img src='data/IHPT.png' style="width:100%; aspect-ratio: 4/3; object-fit: contain; border-radius: 8px;">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://hands-workshop.org/workshop2024.html">
                  <span class="papertitle">Diffusion-based Interacting Hand Pose Transfer</span>
                </a>
                <br>
                <strong>Junho Park*</strong>,
                <a href="https://www.linkedin.com/in/yeieun/">Yeieun Hwang*</a>,
                <a href="https://vds.sogang.ac.kr/">Suk-Ju Kang†</a>
                (*: Equal Contribution, †: Corresponding Author.)
                <br>
                <em>ECCV, 8th Workshop on Observing and Understanding Hands in Action</em>, 2024 &nbsp
                <br>
                <a href="https://hands-workshop.org/files/2024/IHPT__ECCVW_2024_.pdf">Paper</a>
                <p></p>
                <p>
                We propose a new interacting hand pose transfer model, IHPT, which is a diffusion-based approach designed to transfer hand poses between source and target images.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:30%;vertical-align:middle">
                <img src='data/SEMixup.png' style="width:100%; aspect-ratio: 4/3; object-fit: contain; border-radius: 8px;">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/document/10444704">
                  <span class="papertitle">Mixup-based Neural Network for Image Restoration and Structure Prediction from SEM Images</span>
                </a>
                <br>
                <strong>Junho Park</strong>,
                <a href="https://www.linkedin.com/in/yubin-cho-2915a921b/">Yubin Cho</a>,
                <a href="https://www.linkedin.com/in/yeieun/">Yeieun Hwang</a>,
                Ami Ma,
                QHwan Kim,
                Kyu-Baik Chang,
                Jaehoon Jeong,
                <a href="https://vds.sogang.ac.kr/">Suk-Ju Kang†</a>
                (†: Corresponding Author.)
                <br>
                <em>IEEE TIM (Transactions on Instrumentation and Measurement, IF: 5.9)</em>, 2024 &nbsp
                <br>
                <a href="https://ieeexplore.ieee.org/document/10444704">Paper</a>
                <p></p>
                <p>
                We present a new SEM dataset and a two-stage deep learning method (including SEMixup and SEM-SPNet) that achieve state-of-the-art performance in SEM image restoration and structure prediction under diverse conditions.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:30%;vertical-align:middle">
                <img src='data/HANDiffusion.png' style="width:100%; aspect-ratio: 4/3; object-fit: contain; border-radius: 8px;">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://sites.google.com/view/hands2023/abstract-report">
                  <span class="papertitle">A Novel Framework for Generating In-the-Wild 3D Hand Datasets</span>
                </a>
                <br>
                <strong>Junho Park*</strong>,
                <a href="https://www.pnu-cvsp.com/">Kyeongbo Kong*</a>,
                <a href="https://vds.sogang.ac.kr/">Suk-Ju Kang†</a>
                (*: Equal Contribution, †: Corresponding Author.)
                <br>
                <em>ICCV, 7th Workshop on Observing and Understanding Hands in Action</em>, 2023 &nbsp
                <br>
                <a href="https://drive.google.com/file/d/1r3pBVFGSEufzGPhz0l1TmKV_s08FJmP_/view">Paper</a>
                <p></p>
                <p>
                We propose a novel framework, HANDiffusion, which generates new 3D hand datasets with in-the-wild scenes.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:30%;vertical-align:middle">
                <img src='data/SAAF.png' style="width:100%; aspect-ratio: 4/3; object-fit: contain; border-radius: 8px;">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/document/10143197?source=authoralert">
                  <span class="papertitle">Improving Gaze Tracking in Large Screens with Symmetric Gaze Angle Amplification and Optimization Technique</span>
                </a>
                <br>
                <a href="https://www.linkedin.com/in/joseph-kim-b263771b8/">Joseph Kihoon Kim*</a>,
                <strong>Junho Park*</strong>,
                Yeon-Kug Moon†,
                <a href="https://vds.sogang.ac.kr/">Suk-Ju Kang†</a>
                (*: Equal Contribution, †: Corresponding Author.)
                <br>
                <em>IEEE Access (IF: 3.6)</em>, 2023 &nbsp
                <br>
                <a href="https://ieeexplore.ieee.org/document/10143197?source=authoralert">Paper</a>
                <p></p>
                <p>
                We propose a novel gaze tracking method for large screens using a symmetric angle amplifying function and center gravity correction to improve accuracy without personalized calibration, with applications in autonomous vehicles.
                </p>
              </td>
            </tr>

            <!-- <tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='r2r_image'><video  width=100% muted autoplay loop>
                  <source src="images/r2r.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/r2r.jpg' width=100%>
                </div>
                <script type="text/javascript">
                  function r2r_start() {
                    document.getElementById('r2r_image').style.opacity = "1";
                  }

                  function r2r_stop() {
                    document.getElementById('r2r_image').style.opacity = "0";
                  }
                  r2r_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://relight-to-reconstruct.github.io/">
                  <span class="papertitle">Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation</span>
                </a>
                <br>
                <a href="https://hadizayer.github.io/">Hadi Alzayer</a>,
                <a href="https://henzler.github.io/">Philipp Henzler</a>,
                <strong>Jonathan T. Barron</strong>, 
                <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>,
                <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>, 
                <a href="https://dorverbin.github.io/">Dor Verbin</a>
                <br>
                <em>CVPR</em>, 2025 &nbsp <font color=#FF8080><strong>(Highlight)</strong></font>
                <br>
                <a href="https://relight-to-reconstruct.github.io/">project page</a>
                /
                <a href="https://arxiv.org/abs/2412.15211">arXiv</a>
                <p></p>
                <p>
                Images taken under extreme illumination variation can be made consistent with diffusion, and this enables high-quality 3D reconstruction.
                </p>
              </td>
            </tr> -->
          
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Experience</h2>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
              <td style="padding:16px;width:30%;vertical-align:middle">
                <img src='data/vgg_kaist.png' style="width:100%; aspect-ratio: 4/3; object-fit: contain; border-radius: 8px;">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <strong>Collaboration w/ VGG, University of Oxford, United Kingdom & KAIST, South Korea</strong>
                <br>
                Oct. 2024 ~ Present
                <p></p>
                [Under Review] <a href="https://redorangeyellowy.github.io/EgoWorld/">EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations</a>
                <br>
                [ICCV 2025 Workshop] <a href="https://hands-workshop.org/files/2025/Generating.pdf">Generating Egocentric View from Exocentric View via Multimodal Observations</a>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:30%;vertical-align:middle">
                <img src='data/lg.png' style="width:100%; aspect-ratio: 4/3; object-fit: contain; border-radius: 8px;">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <strong>AI Researcher @ LG Electronics, South Korea</strong>
                <br>
                Mar. 2024 ~ Present
                <p></p>
                [Project] Large-Scale Generative Datasets for Vision Tasks
                <br>
                [Project] Vision Foundation Model for On-Device
                <br>
                [Under Review] Single Query to Bind Them: Unified Representations for Efficient Human Pose Estimation
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:30%;vertical-align:middle">
                <img src='data/pnu.png' style="width:100%; aspect-ratio: 4/3; object-fit: contain; border-radius: 8px;">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <strong>Collaboration w/ Pusan National University, South Korea</strong>
                <br>
                Jul. 2023 ~ Feb. 2025
                <p></p>
                [IEEE TMM] <a href="https://jihyun0510.github.io/Programmable_Room_Page/">Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models</a>
                <br>
                [ECCV 2024 Oral] <a href="https://redorangeyellowy.github.io/AttentionHand/">AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild</a>
                <br>
                [ECCV 2024 Workshop Oral] <a href="https://sites.google.com/view/cv4metaverse-2024/program?authuser=0">Interactive 3D Room Generation for Virtual Reality via Compositional Programming</a>
                <br>
                [ICCV 2023 Workshop] <a href="https://sites.google.com/view/hands2023/abstract-report">A Novel Framework for Generating In-the-Wild 3D Hand Datasets</a>
                <br>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:30%;vertical-align:middle">
                <img src='data/samsung.png' style="width:100%; aspect-ratio: 4/3; object-fit: contain; border-radius: 8px;">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <strong>Collaboration w/ Samsung Electronics, South Korea</strong>
                <br>
                Mar. 2023 ~ Feb. 2024
                <p></p>
                [IEEE TIM] <a href="https://ieeexplore.ieee.org/document/10444704">Mixup-based Neural Network for Image Restoration and Structure Prediction from SEM Images</a>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:30%;vertical-align:middle">
                <img src='data/keti.png' style="width:100%; aspect-ratio: 4/3; object-fit: contain; border-radius: 8px;">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <strong>Collaboration w/ Korea Electronics Technology Institute (KETI), South Korea</strong>
                <br>
                Mar. 2022 ~ Feb. 2023
                <p></p>
                [IEEE Access] <a href="https://ieeexplore.ieee.org/document/10143197?source=authoralert">Improving Gaze Tracking in Large Screens With Symmetric Gaze Angle Amplification and Optimization Technique</a>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:30%;vertical-align:middle">
                <img src='data/sgu.png' style="width:100%; aspect-ratio: 4/3; object-fit: contain; border-radius: 8px;">
              </td>
              <td style="padding:8px;width:70%;vertical-align:middle">
                <!-- <strong>Collaboration w/ Sogang University</strong>
                <br>
                Mar. 2024 ~ Present
                <p></p>
                [Under Review]
                <br>
                [ICCV 2025 Workshop]
                <p></p> -->
                <strong>Master Student @ Sogang University, South Korea</strong>
                <!-- <br>
                Feb. 2024 -->
                <p></p>
                [Under Review] TransHOI: Implicit 3D-Aware Cross-View Translation for Hand-Object Interaction Generation
                <br>
                [ICCV 2025 Workshop] <a href="https://hands-workshop.org/files/2025/Replace-in-Ego.pdf">Replace-in-Ego: Text-Guided Object Replacement in Egocentric Hand-Object Interaction</a>
                <br>
                [IEEE TMM] <a href="https://jihyun0510.github.io/Programmable_Room_Page/">Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models</a>
                <br>
                [ECCV 2024 Oral] <a href="https://redorangeyellowy.github.io/AttentionHand/">AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild</a>
                <br>
                [ECCV 2024 Workshop Oral] <a href="https://drive.google.com/file/d/19Proh5jl0Ojz6PI4xJzpI15um2xGofbf/view?usp=drive_link">Interactive 3D Room Generation for Virtual Reality via Compositional Programming</a>
                <br>
                [ECCV 2024 Workshop] <a href="https://hands-workshop.org/files/2024/IHPT__ECCVW_2024_.pdf">Diffusion-based Interacting Hand Pose Transfer</a>
                <br>
                [IEEE TIM] <a href="https://ieeexplore.ieee.org/document/10444704">Mixup-based Neural Network for Image Restoration and Structure Prediction from SEM Images</a>
                <br>
                [ICCV 2023 Workshop] <a href="https://drive.google.com/file/d/1r3pBVFGSEufzGPhz0l1TmKV_s08FJmP_/view">A Novel Framework for Generating In-the-Wild 3D Hand Datasets</a>
                <br>
                [IEEE Access] <a href="https://ieeexplore.ieee.org/document/10143197?source=authoralert">Improving Gaze Tracking in Large Screens With Symmetric Gaze Angle Amplification and Optimization Technique</a>
                <p></p>
                <strong>Undergraduate Student @ Sogang University, South Korea</strong>
                <!-- <br>
                Feb. 2022 -->
                <p></p>
                [Project] <a href="https://cs.sogang.ac.kr/front/cmsboardview.do?currentPage=1&searchField=ALL&searchValue=&searchLowItem=ALL&bbsConfigFK=1749&siteId=cs&pkid=875575">1st place in AI Grand Challenge 2021</a>
              </td>
            </tr>

          </tbody></table>
          
					<!-- <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #fcb97d;">
								 <h2>Micropapers</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr>

            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #aaba9e;">
								 <h2>Recorded Talks</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://www.youtube.com/watch?v=hFlF33JZbA0">Radiance Fields and the Future of Generative Media, 2025</a><br>				  
                <a href="https://www.youtube.com/watch?v=h9vq_65eDas">View Dependent Podcast, 2024</a><br>
                <a href="https://www.youtube.com/watch?v=4tDhYsFuEqo">Bay Area Robotics Symposium, 2023</a><br>
                <a href="https://youtu.be/TvWkwDYtBP4?t=7604">EGSR Keynote, 2021</a><br>
                <a href="https://www.youtube.com/watch?v=nRyOzHpcr4Q">TUM AI Lecture Series, 2020</a><br>
                <a href="https://www.youtube.com/watch?v=HfJpQCBTqZs">Vision & Graphics Seminar at MIT, 2020</a>
              </td>
            </tr>

            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h2>Academic Service</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://iccv.thecvf.com/">Lead Area Chair, ICCV 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2025/Organizers">Lead Area Chair, CVPR 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>
                <br>
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr>
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #edd892;">
								 <h2>Teaching</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
                <br>
                <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
              </td>
            </tr>
            
          </tbody></table> -->
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Website template from <a href="https://jonbarron.info/">Jon Barron.</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
