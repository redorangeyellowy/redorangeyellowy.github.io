<html lang="en">
    <!DOCTYPE HTML>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Junho Park</title>

    <meta name="author" content="Junho Park">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Junho Park
                </p>
                <p>
                  I'm an AI researcher at the Vision Intelligence Lab, led by <a href="https://www.linkedin.com/in/jaechul-kim-56919523/">Jaechul Kim</a>, at <a href="https://www.lge.co.kr/home">LG Electronics</a> in Seoul.
                  At LG Electronics, I've worked on Generative Models (e.g. Diffusion), Vision Foundation Models (e.g. Object Detection, Panoptic Segmentation, Depth Estimation, Pose Estimation, and Multi-Task Learning), and On-Device (e.g. Lightweight Modeling, Quantization).
                  <br><br>
                  I did my Master's program at <a href="https://www.sogang.ac.kr/ko/home/">Sogang University</a> advised by <a href="https://vds.sogang.ac.kr/">Suk-Ju Kang</a>, and heavily co-worked with <a href="https://www.pnu-cvsp.com/">Kyeongbo Kong</a>.
                  At Sogang University, I've worked on 2D/3D Generative Models, Large Language Models, Pose/Gaze Estimation, Quantization, Image Restoration, and Machine Learning.
                  During my two-year Master's program, I published 7 papers (e.g., ECCV Oral, ECCVW, ICCVW, IEEE TMM, IEEE TIM, IEEE Access).
                  <br><br>
                  Additionally, I'm independently pursuing research on Hand-Object Interaction and Egocentric Vision with <a href="https://taeinkwon.com/">Taein Kwon</a>.
                  <br><br>
                  I'm open to collaboration—feel free to reach out!
                </p>
                <p style="text-align:center">
                  <a href="mailto:junho18.park@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/junhopark-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=1OSw79kAAAAJ&hl=ko">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/junho18-park/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/redorangeyellowy/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/profile_dolomites.jpg"><img style="width:100%;max-width:100%;object-fit: cover;" alt="profile photo" src="images/profile_dolomites.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <!-- <p>
                  I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about inferring the physical world (shape, motion, color, light, etc) from images, usually with radiance fields. Some papers are <span class="highlight">highlighted</span>.
                </p> -->
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <!-- <tr onmouseout="cat4d_stop()" onmouseover="cat4d_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='cat4d_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/cat4d.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/attentionhand.png' width="160">
        </div>
        <script type="text/javascript">
          function cat4d_start() {
            document.getElementById('cat4d_image').style.opacity = "1";
          }

          function cat4d_stop() {
            document.getElementById('cat4d_image').style.opacity = "0";
          }
          cat4d_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://redorangeyellowy.github.io/AttentionHand/">
			<span class="papertitle">AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild
      </span>
        </a>
        <br>
				<strong>Junho Park*</strong>,
				<a href="https://scholar.google.com/citations?hl=ko&user=O9QSF7UAAAAJ">Kyeongbo Kong*</a>,
        <a href="https://scholar.google.com/citations?user=3WYxpuYAAAAJ&hl=ko&oi=ao/">Suk-Ju Kang</a>
				<br>
        <em>ECCV</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://redorangeyellowy.github.io/AttentionHand/">Project Page</a>
        /
        <a href="https://arxiv.org/abs/2407.18034">arXiv</a>
        <p></p>
        <p>
				We propose a novel method, AttentionHand, for text-driven controllable hand image generation. The performance of 3D hand mesh reconstruction was improved by additionally training with hand images generated by AttentionHand.
        </p>
      </td>
    </tr> -->


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='images/attentionhand.png' width="160">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://redorangeyellowy.github.io/AttentionHand/">
          <span class="papertitle">AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild</span>
        </a>
        <br>
        <strong>Junho Park*</strong>,
				<a href="https://www.pnu-cvsp.com/">Kyeongbo Kong*</a>,
        <a href="https://vds.sogang.ac.kr/">Suk-Ju Kang†</a>
				<br>
        <em>ECCV</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://redorangeyellowy.github.io/AttentionHand/">Project Page</a>
        /
        <a href="https://arxiv.org/abs/2407.18034">arXiv</a>
        <p></p>
        <p>
				We propose a novel method, AttentionHand, for text-driven controllable hand image generation. The performance of 3D hand mesh reconstruction was improved by additionally training with hand images generated by AttentionHand.
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='images/attentionhand.png' width="160">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://sites.google.com/view/cv4metaverse-2024/program?authuser=0">
          <span class="papertitle">Interactive 3D Room Generation for Virtual Reality via Compositional Programming</span>
        </a>
        <br>
        <a href="https://www.linkedin.com/in/%EC%A7%80%ED%98%84-%EA%B9%80-a994171b5/">Jihyun Kim*</a>,
        <strong>Junho Park*</strong>,
				<a href="https://www.pnu-cvsp.com/">Kyeongbo Kong*</a>,
        <a href="https://vds.sogang.ac.kr/">Suk-Ju Kang†</a>
				<br>
        <em>ECCVW</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://sites.google.com/view/cv4metaverse-2024/program?authuser=0">Extended Abstract</a>
        <p></p>
        <p>
				We propose a novel method, AttentionHand, for text-driven controllable hand image generation. The performance of 3D hand mesh reconstruction was improved by additionally training with hand images generated by AttentionHand.
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='images/attentionhand.png' width="160">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://hands-workshop.org/workshop2024.html">
          <span class="papertitle">Diffusion-based Interacting Hand Pose Transfer</span>
        </a>
        <br>
        <strong>Junho Park*</strong>,
				<a href="https://www.linkedin.com/in/yeieun/">Yeieun Hwang*</a>,
        <a href="https://vds.sogang.ac.kr/">Suk-Ju Kang†</a>
				<br>
        <em>ECCVW</em>, 2024 &nbsp
        <br>
        <a href="https://hands-workshop.org/files/2024/IHPT__ECCVW_2024_.pdf">Extended Abstract</a>
        <p></p>
        <p>
				 We propose a new interacting hand pose transfer model, IHPT, which is a diffusion-based approach designed to transfer hand poses between source and target images. IHPT can generate a new target image with the target hand pose while maintaining the source image’s texture and quality, leading to improved semantic understanding and generalizability in generating target hand poses.
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='images/attentionhand.png' width="160">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://redorangeyellowy.github.io/AttentionHand/">
          <span class="papertitle">AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild</span>
        </a>
        <br>
        <strong>Junho Park*</strong>,
				<a href="https://www.pnu-cvsp.com/">Kyeongbo Kong*</a>,
        <a href="https://vds.sogang.ac.kr/">Suk-Ju Kang†</a>
				<br>
        <em>ECCV</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://redorangeyellowy.github.io/AttentionHand/">Project Page</a>
        /
        <a href="https://arxiv.org/abs/2407.18034">arXiv</a>
        <p></p>
        <p>
				We propose a novel method, AttentionHand, for text-driven controllable hand image generation. The performance of 3D hand mesh reconstruction was improved by additionally training with hand images generated by AttentionHand.
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='images/attentionhand.png' width="160">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://redorangeyellowy.github.io/AttentionHand/">
          <span class="papertitle">AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild</span>
        </a>
        <br>
        <strong>Junho Park*</strong>,
				<a href="https://www.pnu-cvsp.com/">Kyeongbo Kong*</a>,
        <a href="https://vds.sogang.ac.kr/">Suk-Ju Kang†</a>
				<br>
        <em>ECCV</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://redorangeyellowy.github.io/AttentionHand/">Project Page</a>
        /
        <a href="https://arxiv.org/abs/2407.18034">arXiv</a>
        <p></p>
        <p>
				We propose a novel method, AttentionHand, for text-driven controllable hand image generation. The performance of 3D hand mesh reconstruction was improved by additionally training with hand images generated by AttentionHand.
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:20%;vertical-align:middle">
        <img src='images/attentionhand.png' width="160">
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://redorangeyellowy.github.io/AttentionHand/">
          <span class="papertitle">AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild</span>
        </a>
        <br>
        <strong>Junho Park*</strong>,
				<a href="https://www.pnu-cvsp.com/">Kyeongbo Kong*</a>,
        <a href="https://vds.sogang.ac.kr/">Suk-Ju Kang†</a>
				<br>
        <em>ECCV</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://redorangeyellowy.github.io/AttentionHand/">Project Page</a>
        /
        <a href="https://arxiv.org/abs/2407.18034">arXiv</a>
        <p></p>
        <p>
				We propose a novel method, AttentionHand, for text-driven controllable hand image generation. The performance of 3D hand mesh reconstruction was improved by additionally training with hand images generated by AttentionHand.
        </p>
      </td>
    </tr>



    <!-- <tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='r2r_image'><video  width=100% muted autoplay loop>
          <source src="images/r2r.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/r2r.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function r2r_start() {
            document.getElementById('r2r_image').style.opacity = "1";
          }

          function r2r_stop() {
            document.getElementById('r2r_image').style.opacity = "0";
          }
          r2r_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://relight-to-reconstruct.github.io/">
          <span class="papertitle">Generative Multiview Relighting for
3D Reconstruction under Extreme Illumination Variation</span>
        </a>
        <br>
        <a href="https://hadizayer.github.io/">Hadi Alzayer</a>,
        <a href="https://henzler.github.io/">Philipp Henzler</a>,
				<strong>Jonathan T. Barron</strong>, 
        <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>,
        <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>, 
        <a href="https://dorverbin.github.io/">Dor Verbin</a>
        <br>
        <em>CVPR</em>, 2025 &nbsp <font color=#FF8080><strong>(Highlight)</strong></font>
        <br>
        <a href="https://relight-to-reconstruct.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2412.15211">arXiv</a>
        <p></p>
        <p>
				Images taken under extreme illumination variation can be made consistent with diffusion, and this enables high-quality 3D reconstruction.
        </p>
      </td>
    </tr> -->



          </tbody></table>

          
					<table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #fcb97d;">
								 <h2>Micropapers</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr>


            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #aaba9e;">
								 <h2>Recorded Talks</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://www.youtube.com/watch?v=hFlF33JZbA0">Radiance Fields and the Future of Generative Media, 2025</a><br>				  
                <a href="https://www.youtube.com/watch?v=h9vq_65eDas">View Dependent Podcast, 2024</a><br>
                <a href="https://www.youtube.com/watch?v=4tDhYsFuEqo">Bay Area Robotics Symposium, 2023
</a><br>
                <a href="https://youtu.be/TvWkwDYtBP4?t=7604">EGSR Keynote, 2021</a><br>
				<a href="https://www.youtube.com/watch?v=nRyOzHpcr4Q">TUM AI Lecture Series, 2020</a><br>
				<a href="https://www.youtube.com/watch?v=HfJpQCBTqZs">Vision & Graphics Seminar at MIT, 2020</a>
              </td>
            </tr>

            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h2>Academic Service</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://iccv.thecvf.com/">Lead Area Chair, ICCV 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2025/Organizers">Lead Area Chair, CVPR 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>
                <br>
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr>
						
						
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #edd892;">
								 <h2>Teaching</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
                <br>
                <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
              </td>
            </tr>
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
